(window.webpackJsonp=window.webpackJsonp||[]).push([[565],{879:function(s,a,t){"use strict";t.r(a);var e=t(7),n=Object(e.a)({},(function(){var s=this,a=s._self._c;return a("ContentSlotsDistributor",{attrs:{"slot-key":s.$parent.slotKey}},[a("h1",{attrs:{id:"hive-分区表和分桶表"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#hive-分区表和分桶表"}},[s._v("#")]),s._v(" Hive 分区表和分桶表")]),s._v(" "),a("h2",{attrs:{id:"分区表"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#分区表"}},[s._v("#")]),s._v(" 分区表")]),s._v(" "),a("h3",{attrs:{id:"概念"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#概念"}},[s._v("#")]),s._v(" 概念")]),s._v(" "),a("p",[s._v("Hive 中的表对应为 HDFS 上的指定目录，在查询数据时候，默认会对全表进行扫描，这样时间和性能的消耗都非常大。")]),s._v(" "),a("p",[a("strong",[s._v("分区为 HDFS 上表目录的子目录")]),s._v("，数据按照分区存储在子目录中。如果查询的 "),a("code",[s._v("where")]),s._v(" 子句中包含分区条件，则直接从该分区去查找，而不是扫描整个表目录，合理的分区设计可以极大提高查询速度和性能。")]),s._v(" "),a("blockquote",[a("p",[s._v("这里说明一下分区表并非 Hive 独有的概念，实际上这个概念非常常见。比如在我们常用的 Oracle 数据库中，当表中的数据量不断增大，查询数据的速度就会下降，这时也可以对表进行分区。表进行分区后，逻辑上表仍然是一张完整的表，只是将表中的数据存放到多个表空间（物理文件上），这样查询数据时，就不必要每次都扫描整张表，从而提升查询性能。")])]),s._v(" "),a("h3",{attrs:{id:"使用场景"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#使用场景"}},[s._v("#")]),s._v(" 使用场景")]),s._v(" "),a("p",[s._v("通常，在管理大规模数据集的时候都需要进行分区，比如将日志文件按天进行分区，从而保证数据细粒度的划分，使得查询性能得到提升。")]),s._v(" "),a("h3",{attrs:{id:"创建分区表"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#创建分区表"}},[s._v("#")]),s._v(" 创建分区表")]),s._v(" "),a("p",[s._v("在 Hive 中可以使用 "),a("code",[s._v("PARTITIONED BY")]),s._v(" 子句创建分区表。表可以包含一个或多个分区列，程序会为分区列中的每个不同值组合创建单独的数据目录。下面的我们创建一张雇员表作为测试：")]),s._v(" "),a("div",{staticClass:"language-sql line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("CREATE")]),s._v(" EXTERNAL "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("TABLE")]),s._v(" emp_partition"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("\n    empno "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("INT")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n    ename STRING"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n    job STRING"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n    mgr "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("INT")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n    hiredate "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("TIMESTAMP")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n    sal "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("DECIMAL")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("7")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n    comm "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("DECIMAL")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("7")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    PARTITIONED "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("BY")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("deptno "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("INT")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("   "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("-- 按照部门编号进行分区")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("ROW")]),s._v(" FORMAT DELIMITED "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("FIELDS")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("TERMINATED")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("BY")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"\\t"')]),s._v("\n    LOCATION "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'/hive/emp_partition'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br")])]),a("h3",{attrs:{id:"加载数据到分区表"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#加载数据到分区表"}},[s._v("#")]),s._v(" 加载数据到分区表")]),s._v(" "),a("p",[s._v("加载数据到分区表时候必须要指定数据所处的分区：")]),s._v(" "),a("div",{staticClass:"language-shell line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 加载部门编号为20的数据到表中")]),s._v("\nLOAD DATA LOCAL INPATH "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"/usr/file/emp20.txt"')]),s._v(" OVERWRITE INTO TABLE emp_partition PARTITION "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("deptno"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("20")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 加载部门编号为30的数据到表中")]),s._v("\nLOAD DATA LOCAL INPATH "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"/usr/file/emp30.txt"')]),s._v(" OVERWRITE INTO TABLE emp_partition PARTITION "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("deptno"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("30")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br")])]),a("h3",{attrs:{id:"查看分区目录"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#查看分区目录"}},[s._v("#")]),s._v(" 查看分区目录")]),s._v(" "),a("p",[s._v("这时候我们直接查看表目录，可以看到表目录下存在两个子目录，分别是 "),a("code",[s._v("deptno=20")]),s._v(" 和 "),a("code",[s._v("deptno=30")]),s._v(",这就是分区目录，分区目录下才是我们加载的数据文件。")]),s._v(" "),a("div",{staticClass:"language-shell line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# hadoop fs -ls  hdfs://hadoop001:8020/hive/emp_partition/")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("这时候当你的查询语句的 "),a("code",[s._v("where")]),s._v(" 包含 "),a("code",[s._v("deptno=20")]),s._v("，则就去对应的分区目录下进行查找，而不用扫描全表。")]),s._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/heibaiying/BigData-Notes/raw/master/pictures/hive-hadoop-partitation.png",alt:"img"}})]),s._v(" "),a("h2",{attrs:{id:"分桶表"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#分桶表"}},[s._v("#")]),s._v(" 分桶表")]),s._v(" "),a("h3",{attrs:{id:"简介"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#简介"}},[s._v("#")]),s._v(" 简介")]),s._v(" "),a("p",[s._v("分区提供了一个隔离数据和优化查询的可行方案，但是并非所有的数据集都可以形成合理的分区，分区的数量也不是越多越好，过多的分区条件可能会导致很多分区上没有数据。同时 Hive 会限制动态分区可以创建的最大分区数，用来避免过多分区文件对文件系统产生负担。鉴于以上原因，Hive 还提供了一种更加细粒度的数据拆分方案：分桶表 (bucket Table)。")]),s._v(" "),a("p",[s._v("分桶表会将指定列的值进行哈希散列，并对 bucket（桶数量）取余，然后存储到对应的 bucket（桶）中。")]),s._v(" "),a("h3",{attrs:{id:"理解分桶表"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#理解分桶表"}},[s._v("#")]),s._v(" 理解分桶表")]),s._v(" "),a("p",[s._v("单从概念上理解分桶表可能会比较晦涩，其实和分区一样，分桶这个概念同样不是 Hive 独有的，对于 Java 开发人员而言，这可能是一个每天都会用到的概念，因为 Hive 中的分桶概念和 Java 数据结构中的 HashMap 的分桶概念是一致的。")]),s._v(" "),a("p",[s._v("当调用 HashMap 的 put() 方法存储数据时，程序会先对 key 值调用 hashCode() 方法计算出 hashcode，然后对数组长度取模计算出 index，最后将数据存储在数组 index 位置的链表上，链表达到一定阈值后会转换为红黑树 (JDK1.8+)。下图为 HashMap 的数据结构图：")]),s._v(" "),a("p",[a("img",{attrs:{src:"https://www.yuyanqing.cn/oss/image-bed/snap/20200224194352.png",alt:"img"}})]),s._v(" "),a("p",[s._v("图片引用自："),a("a",{attrs:{href:"http://www.itcuties.com/java/hashmap-hashtable/",target:"_blank",rel:"noopener noreferrer"}},[s._v("HashMap vs. Hashtable"),a("OutboundLink")],1)]),s._v(" "),a("h3",{attrs:{id:"创建分桶表"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#创建分桶表"}},[s._v("#")]),s._v(" 创建分桶表")]),s._v(" "),a("p",[s._v("在 Hive 中，我们可以通过 "),a("code",[s._v("CLUSTERED BY")]),s._v(" 指定分桶列，并通过 "),a("code",[s._v("SORTED BY")]),s._v(" 指定桶中数据的排序参考列。下面为分桶表建表语句示例：")]),s._v(" "),a("div",{staticClass:"language-sql line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[s._v("  "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("CREATE")]),s._v(" EXTERNAL "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("TABLE")]),s._v(" emp_bucket"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("\n    empno "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("INT")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n    ename STRING"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n    job STRING"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n    mgr "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("INT")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n    hiredate "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("TIMESTAMP")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n    sal "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("DECIMAL")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("7")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n    comm "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("DECIMAL")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("7")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n    deptno "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("INT")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("CLUSTERED")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("BY")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("empno"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" SORTED "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("BY")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("empno "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("ASC")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("INTO")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("4")]),s._v(" BUCKETS  "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("--按照员工编号散列到四个 bucket 中")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("ROW")]),s._v(" FORMAT DELIMITED "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("FIELDS")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("TERMINATED")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("BY")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"\\t"')]),s._v("\n    LOCATION "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'/hive/emp_bucket'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br")])]),a("h3",{attrs:{id:"加载数据到分桶表"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#加载数据到分桶表"}},[s._v("#")]),s._v(" 加载数据到分桶表")]),s._v(" "),a("p",[s._v("这里直接使用 "),a("code",[s._v("Load")]),s._v(" 语句向分桶表加载数据，数据时可以加载成功的，但是数据并不会分桶。")]),s._v(" "),a("p",[s._v("这是由于分桶的实质是对指定字段做了 hash 散列然后存放到对应文件中，这意味着向分桶表中插入数据是必然要通过 MapReduce，且 Reducer 的数量必须等于分桶的数量。由于以上原因，分桶表的数据通常只能使用 CTAS(CREATE TABLE AS SELECT) 方式插入，因为 CTAS 操作会触发 MapReduce。加载数据步骤如下：")]),s._v(" "),a("h4",{attrs:{id:"设置强制分桶"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#设置强制分桶"}},[s._v("#")]),s._v(" 设置强制分桶")]),s._v(" "),a("div",{staticClass:"language-sql line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("set")]),s._v(" hive"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("enforce"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("bucketing "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("true")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("--Hive 2.x 不需要这一步")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("在 Hive 0.x and 1.x 版本，必须使用设置 "),a("code",[s._v("hive.enforce.bucketing = true")]),s._v("，表示强制分桶，允许程序根据表结构自动选择正确数量的 Reducer 和 cluster by column 来进行分桶。")]),s._v(" "),a("h4",{attrs:{id:"ctas-导入数据"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#ctas-导入数据"}},[s._v("#")]),s._v(" CTAS 导入数据")]),s._v(" "),a("div",{staticClass:"language-sql line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("INSERT")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("INTO")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("TABLE")]),s._v(" emp_bucket "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("SELECT")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("*")]),s._v("  "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("FROM")]),s._v(" emp"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("--这里的 emp 表就是一张普通的雇员表")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("可以从执行日志看到 CTAS 触发 MapReduce 操作，且 Reducer 数量和建表时候指定 bucket 数量一致：")]),s._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/heibaiying/BigData-Notes/raw/master/pictures/hive-hadoop-mapreducer.png",alt:"img"}})]),s._v(" "),a("h3",{attrs:{id:"查看分桶文件"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#查看分桶文件"}},[s._v("#")]),s._v(" 查看分桶文件")]),s._v(" "),a("p",[s._v("bucket(桶) 本质上就是表目录下的具体文件：")]),s._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/heibaiying/BigData-Notes/raw/master/pictures/hive-hadoop-bucket.png",alt:"img"}})]),s._v(" "),a("h2",{attrs:{id:"分区表和分桶表结合使用"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#分区表和分桶表结合使用"}},[s._v("#")]),s._v(" 分区表和分桶表结合使用")]),s._v(" "),a("p",[s._v("分区表和分桶表的本质都是将数据按照不同粒度进行拆分，从而使得在查询时候不必扫描全表，只需要扫描对应的分区或分桶，从而提升查询效率。两者可以结合起来使用，从而保证表数据在不同粒度上都能得到合理的拆分。下面是 Hive 官方给出的示例：")]),s._v(" "),a("div",{staticClass:"language-sql line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("CREATE")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("TABLE")]),s._v(" page_view_bucketed"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("\n\tviewTime "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("INT")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n    userid "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("BIGINT")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n    page_url STRING"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n    referrer_url STRING"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n    ip STRING "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n PARTITIONED "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("BY")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("dt STRING"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("CLUSTERED")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("BY")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("userid"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" SORTED "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("BY")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("viewTime"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("INTO")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("32")]),s._v(" BUCKETS\n "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("ROW")]),s._v(" FORMAT DELIMITED\n   "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("FIELDS")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("TERMINATED")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("BY")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'\\001'")]),s._v("\n   COLLECTION ITEMS "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("TERMINATED")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("BY")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'\\002'")]),s._v("\n   MAP "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("KEYS")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("TERMINATED")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("BY")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'\\003'")]),s._v("\n STORED "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("AS")]),s._v(" SEQUENCEFILE"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br")])]),a("p",[s._v("此时导入数据时需要指定分区：")]),s._v(" "),a("div",{staticClass:"language-shell line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[s._v("INSERT OVERWRITE page_view_bucketed\nPARTITION "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("dt"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'2009-02-25'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\nSELECT * FROM page_view WHERE "),a("span",{pre:!0,attrs:{class:"token assign-left variable"}},[s._v("dt")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'2009-02-25'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br")])]),a("h2",{attrs:{id:"参考资料"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#参考资料"}},[s._v("#")]),s._v(" 参考资料")]),s._v(" "),a("ul",[a("li",[a("a",{attrs:{href:"https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL+BucketedTables",target:"_blank",rel:"noopener noreferrer"}},[s._v("LanguageManual DDL BucketedTables"),a("OutboundLink")],1)])])])}),[],!1,null,null,null);a.default=n.exports}}]);